{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Text-Generating Model w/ GPU on Love Letter",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fillerInk/love-letters/blob/master/GPT_2_Text_Generating_Model_w_GPU_on_Love_Letter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "5d13a422-6eff-40f6-f427-9a3e764f45dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "5c957e0c-6bc0-4077-f9b1-7394ca02fc6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 30 15:54:39 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "ea75502b-b8d3-4986-c614-bb1cf471f7f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 323Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 116Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 375Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:13, 107Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 260Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 141Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 192Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "efb38118-5c5b-42f1-9782-3f5c45ae9e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"letters_v1.1.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "519288ea-dc7a-4d20-ea99-52077e2597df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=500,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=100,\n",
        "              save_every=250,\n",
        "              learning_rate=1e-5\n",
        "              )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 24893 tokens\n",
            "Training...\n",
            "[10 | 16.54] loss=3.98 avg=3.98\n",
            "[20 | 25.37] loss=3.38 avg=3.68\n",
            "[30 | 34.22] loss=3.59 avg=3.65\n",
            "[40 | 43.06] loss=3.32 avg=3.57\n",
            "[50 | 51.92] loss=3.18 avg=3.49\n",
            "[60 | 60.77] loss=3.07 avg=3.41\n",
            "[70 | 69.63] loss=3.48 avg=3.42\n",
            "[80 | 78.47] loss=3.01 avg=3.37\n",
            "[90 | 87.31] loss=2.93 avg=3.32\n",
            "[100 | 96.15] loss=2.83 avg=3.27\n",
            "======== SAMPLE 1 ========\n",
            " nothing but good.\"\n",
            "\n",
            "\n",
            "The best part of all of this is that the woman was actually the one who wrote this story. It was she who said that the story's setting and setting alone could possibly create a story. We can be sure that she would have thought that this is precisely the kind of tale she would write. She said at a meeting of the Association of American Publishers, of all places…\n",
            "\n",
            "\n",
            "\"I believe I have brought into reality a few of the wild projects which have formed a part of my own life and that of most women, and which the male author might well have regarded as the province of an idle imagination.\" And I thought of all the stories to be written by her and of all the women who have written to me, and of all the wonderful combinations of all these, and I have found nothing but love and kindness! I have loved and loved you, and that is the most important thing. I have loved and loved you—but you have been gone from here all these days. Your spirit was gone as a breeze through a cloud, and I came to you as a maiden who has fallen in love.\n",
            "\n",
            "\n",
            "It is always with me that I remember what you said:\n",
            "\n",
            "\n",
            "\"My dear, I cannot bear to be alone on the journey alone, for I know how much I love you and I always think of the time when I would make your happiness our duty.\"\n",
            "\n",
            "It is sometimes with me that I wish every moment that you could be one of the things that I love so much.\n",
            "\n",
            "\n",
            "The man who loves someone who is less than they are cannot love them.\n",
            "\n",
            "\n",
            "It is only when we are on the road that we discover all that we have forgotten.\n",
            "\n",
            "\n",
            "You gave so much of yourself in such brief a time: you were only able to give so little.\n",
            "\n",
            "\n",
            "My love, always, you are there with me, always and always.\n",
            "\n",
            "\n",
            "The moment that I knew you no more, the moment that came when I felt your gentle touch on my eyes, my tender lips, my bright eyes, my firm heart and that beautiful face, I could never again look away from you.\n",
            "\n",
            "\n",
            "What did that beautiful face and heart mean to you? What did I mean when I wrote such beautiful words about you?\n",
            "\n",
            "As a lover I have a sense of how strong you are, of what you are worth and what you can never lose, even if you die before you can realize it. And then my eyes are upon you, and as I look upon you as I would the most beautiful flower in a field, all the beauty and tenderness I feel for you is for ever with me.\n",
            "\n",
            "\n",
            "I love you always and I do the love that is most likely to bring about my downfall.\n",
            "\n",
            "Do you remember all those little dances —\n",
            "\n",
            "\n",
            "And then, when you were very young and I was a great father in the world, I went to you with my eyes, and all my kisses —\n",
            "\n",
            "When, when, when, I went to you. Love is strong, I have sworn.\n",
            "\n",
            "\n",
            "As we are traveling, your sweetest smile has a bright, glowing, happy quality about it, like the rays of the sunrise from heaven, just like sunshine in the midst of a cloudless sky.\n",
            "\n",
            "Let me rest in your arms, and I will not be weary.\n",
            "\n",
            "\n",
            "I believe that the very next thing that your loving, wonderful face told me was,\n",
            "\n",
            "That I am the very picture of beauty when nothing else is.\n",
            "\n",
            "\n",
            "It is always so with men. But even so, let it always be the case, that we, men too, will always say to all other men,\n",
            "\n",
            "I, who have so many eyes on me, I, I, I,\n",
            "\n",
            "Who have so many lives in me, but who must be destroyed at the outset if I am ever a part of yourself,\n",
            "\n",
            "I, I, the whole is a farce.\n",
            "\n",
            "\n",
            "And then will we all look upon one another as we look up to the stars, all over and over again.\n",
            "\n",
            "You, my boy, my shining angel, my radiant love.\n",
            "\n",
            "\n",
            "Dear old Henry the Wise,\n",
            "I thank you for your letter. I am so very sorry that you cannot live another day and I am so very sorry that you can not remain a thousand years more. But I am afraid that I cannot ever put an end to your story: I have to live to find out how far I have gone. I have already tried everything.\n",
            "\n",
            "If we lived at present, I think we had a perfectly happy whole life and I am not sure that I am that sort of man, you have the satisfaction of knowing that I am now sure of nothing.\n",
            "\n",
            "At the moment I am so perfectly sure that my little one is as perfectly good and healthy as she ever was and I am as certain that she never will ever be able to live a good life again unless I put a stop to this miserable\n",
            "\n",
            "[110 | 123.99] loss=3.12 avg=3.25\n",
            "[120 | 132.84] loss=2.90 avg=3.22\n",
            "[130 | 141.70] loss=2.65 avg=3.18\n",
            "[140 | 150.54] loss=3.12 avg=3.17\n",
            "[150 | 159.39] loss=2.33 avg=3.11\n",
            "[160 | 168.23] loss=2.69 avg=3.08\n",
            "[170 | 177.06] loss=2.02 avg=3.02\n",
            "[180 | 185.89] loss=2.57 avg=2.99\n",
            "[190 | 194.73] loss=2.34 avg=2.95\n",
            "[200 | 203.58] loss=2.60 avg=2.93\n",
            "======== SAMPLE 1 ========\n",
            " though he's one of my favorite filmmakers as well as a friend and I'm just super excited to see what goes into this next movie. He's also such a great guy and artist.\n",
            "So yeah to all the lovely people at all the shoots and events and to my dear fans and so many other amazing people at different levels, thank you so much, and we can't wait to see you as there might be a few tears 😢<|endoftext|>The \"Goddess\" with the Darkly Satanic Hair\n",
            "\n",
            "She is the goddess with darkly Satanic hair.\n",
            "\n",
            "When one looks closely at her, one is struck by her supernatural power. If this power were real, she would be able to change her form in the slightest moment, leaving nothing but darkness and confusion in her wake.<|endoftext|>What a great time to have it! My santa got me TWO delicious items this Christmas! One was a beautiful mug, a beautiful chocolate chip cookie. My santa also got me a book about chocolate that will definitely help me remember Christmas in the best way possible.\n",
            "\n",
            "I hope you all had an awesome Christmas and I will be posting pictures and other interesting things on my profile to make it as fun and interesting as possible for my amazing Santa!<|endoftext|>WASHINGTON — The House Intelligence Committee voted Wednesday to ask officials involved in the FBI investigation of Hillary Clinton about a private email server she used while secretary of State.\n",
            "\n",
            "Committee Chairman Devin Nunes, R-Calif., made his announcement after a long interview with FBI Director James B. Comey as part of its investigation into Russian interference in the 2016 election and possible links between the Trump campaign and Moscow.\n",
            "\n",
            "The news was seen by some supporters of Mrs. Obama as an indication of the seriousness of their investigation.\n",
            "\n",
            "The two-decade Senate investigation into the matter remains open.<|endoftext|>After a decade of fighting on our side of the world, many of you, my friends, have grown tired of the war.\n",
            "I want to tell you about how I felt when the war ended – it is a powerful word.\n",
            "I was 22 when the war ended in 1991.\n",
            "Two years earlier, with my family in Australia and Canada, I had come here with the dream of meeting many of the people and experiencing the country, of making new friends. For the first time in the war, I had the honour of visiting a country where a Japanese attack on Pearl Harbour had killed two-thirds of the world's men. And it was here that I finally learnt the truth of the story the US government has been peddling to us – that our only hope of defeating the enemy lies in the unconditional surrender of Japan; and that we will never win the war, since the only way to do so is to end the war upon friendly terms.\n",
            "The truth is much the same for all of us, of course, who have lived through wars. We hear reports of carnage. We read of the suffering of millions of our men, women and children in our own countries. We know the terrible price in human blood and treasure to be exacted, even at the cost of peace and prosperity.\n",
            "But we are also certain that in every situation where human life is at stake, there comes a moment, when what can possibly be called weakness or even cowardice has to be called on to the stand – when a call must be made, which has been made for us for more than seven decades by our great leaders, almost all of Japanese descent.\n",
            "The leaders of the Government of Japan and of the Government of the People's Republic of China have always been on the same side.\n",
            "If they were a third of our people, then in the last war I myself had long been a supporter of the Government of the People's Republic of China, knowing that if we fought together the Japanese would have to give up the one thing that separated us from their entire race – the country they own so carefully and so completely.\n",
            "The Japanese, from their very first day in our soil, have fought to the very end to obtain their rightful place in the world. We in Japan never cease to honour and revere the ancestors who came before us, to love and cherish them, even to the point of destroying any of our children who might be born in our society.\n",
            "It would be impossible to mention here the many great victories the Japanese have won in the world war; but their most important contribution to the cause of peace which we have so many years now worked towards – and towards which we have so little success – is our unconditional acceptance of their responsibility for the crimes they have committed against humanity in the past, and for the crimes that they will undoubtedly commit as long as they are permitted by human nature to remain in their present position on this planet.\n",
            "The war ended the same way and for the same cause for Japan and for the Government of Japan. Our leaders had promised us – and I and my cabinet and the entire Japanese people have always supported these promises – that we should win the war only if the Japanese renounced possession of our islands and\n",
            "\n",
            "[210 | 229.12] loss=2.45 avg=2.91\n",
            "[220 | 237.96] loss=1.61 avg=2.84\n",
            "[230 | 246.80] loss=1.23 avg=2.76\n",
            "[240 | 255.63] loss=2.65 avg=2.76\n",
            "[250 | 264.47] loss=1.10 avg=2.68\n",
            "Saving checkpoint/run1/model-250\n",
            "[260 | 280.84] loss=3.00 avg=2.70\n",
            "[270 | 289.70] loss=2.61 avg=2.69\n",
            "[280 | 298.55] loss=1.83 avg=2.66\n",
            "[290 | 307.40] loss=1.85 avg=2.63\n",
            "[300 | 316.23] loss=1.93 avg=2.60\n",
            "======== SAMPLE 1 ========\n",
            "com\n",
            "http://sansantonio.tumblr.com/\n",
            "https://www.facebook.com/pages/Sans-Panto/18073548359981\n",
            "All artwork and sketches are by me. I thank you!<|endoftext|>This is a post in our new Community Blog series, where we try to answer your recurring questions and find topics across all of our business to grow.\n",
            "Today we are diving deep into one of the hottest new social network marketplaces – Facebook.com.\n",
            "There are a huge variety of apps, products, and services on offer and we want you to be able to find the one which is best for you, your business and your community.\n",
            "You have the power to select a product. We have a product and we will make it better. In this way, you will learn something new and we will grow together. What we don't know quite yet is how to share what we do know, and why you should trust us.\n",
            "We won't tell you how to spend your cash yet but we will let you in on a little secret…\n",
            "We earn 35% of every sale you make on our site.\n",
            "What does this mean? It is our share of all your money, just like you earn from every sale.\n",
            "Here is a chart showing how much we earn.\n",
            "We never receive any of your money, it never leaves our hands and we cannot earn what you do earn.\n",
            "So what is all this fuss about earning some extra cash?\n",
            "A lot of things. Not the least of which is the enormous volume of traffic we are getting every minute.\n",
            "We now have hundreds of thousands of fans around the world – thousands of hours watched, hours of conversation.\n",
            "And you – our amazing, devoted, loyal, creative and engaged 30 million Facebook fans and 14 million Twitter fans – are simply incredible.\n",
            "We can't express how much we love this group of people.\n",
            "You are amazing.\n",
            "We are not here to write letters, we have been sending and receiving lots of them. Many people have written to me about the wonderful things they are seeing and hearing about us…but you must read what I have to say to believe it.\n",
            "We must try as much as we can not to let what is written, heard or heard over our heads, but we can't keep quiet.\n",
            "If we speak frankly, if we act honestly, we shall find that you are more than all that. And if we do nothing else, if we act only in words, we shall not only fail in our task, we shall almost certainly not even live to see it completed.\n",
            "Read on, O people! If you have not already, go to the Facebook page of your favourite writer, try out his or her name. Then contact him or her, telling him or her about your love for him or her and asking him or her to write about you. Do this four or five times in two months, sending a short letter, asking him or her to keep you posted. Then send another letter to a third person, saying that you have got his or her name in order and asking him or her to write to him or her.\n",
            "Once you have done all this, if you are kind to people, faithful to the law and friendly to one another, you will almost certainly live to see our publication.\n",
            "But do not let us discourage you – we shall keep on encouraging you.\n",
            "We know that the first issue contains only one poem – we invite you to pick as many as you like. Then we shall have the consolation of knowing that for our part we cannot pay the vast cost of the printing and publication of Love Poems.\n",
            "Because they are for us, they are sacred. And because they are so, we have great pleasure in offering them to you.\n",
            "And, most particularly, we have great pleasure in asking that you please to join us on this most sacred day of all: 25 August.\n",
            "Mary\n",
            "Dear Mary: It has been a long time since I wrote to you. Perhaps you do not know how many years have passed since I left Concord…\n",
            "Perhaps you don't know that I moved every evening, at three o'clock, to my room at St. John's, leaving behind me only my ring and a few cards and little notes.\n",
            "I never written a letter to you, I never left you a note, so please write to me. The more clearly you see yourself, the better it is for me to answer you…\n",
            "As for the ring I promised myself I would give it to my little brother. I think he likes it very much.\n",
            "I like the ring very much. As soon as I put it on I climb into the drawing-room window and look out upon all this marvelous Garden, and I think: 'I shall live there all my days, and not once will I look on death, lest I should envy those who live.'\n",
            "I have tried to live the whole of my days like that. How much more strongly\n",
            "\n",
            "[310 | 341.76] loss=1.37 avg=2.55\n",
            "[320 | 350.61] loss=1.14 avg=2.50\n",
            "[330 | 359.44] loss=2.03 avg=2.49\n",
            "[340 | 368.29] loss=1.48 avg=2.45\n",
            "[350 | 377.14] loss=0.50 avg=2.39\n",
            "[360 | 385.97] loss=1.38 avg=2.35\n",
            "[370 | 394.82] loss=1.64 avg=2.33\n",
            "[380 | 403.65] loss=0.50 avg=2.27\n",
            "[390 | 412.50] loss=0.43 avg=2.21\n",
            "[400 | 421.33] loss=0.79 avg=2.17\n",
            "======== SAMPLE 1 ========\n",
            " should we allow them to take such power over our minds and bodies? For all their talk of the rights and wrongs of the family, religion, and the civil law, they keep silence when they ought to speak; they are silent where there is no government — in our hearts and souls. There is no government but by love — love that obeys neither chance nor chance alone — love that never falters in demanding what is due and expecting more of us — that we are its equal — that our loyalty is its first loyalty — and that we shall continue to be.\n",
            "\n",
            "One thing more. There was once on this continent a people whose institutions were free — whose laws were inviolable — whose government was inviolable — whose people were free — — — — —\n",
            "\n",
            "You have violated — — —\n",
            "\n",
            "them. You have robbed them — you have usurped their rights — you have trampled upon their independence.\n",
            "\n",
            "My beloved friends,\n",
            "\n",
            "I have never, under any circumstances, believed in Nature as she operates — I have regarded her with an almost scientific eye, and I have judged of her on the principles laid down for me by Nature.\n",
            "\n",
            "But what has been the work of my labours as an individual? —What has been the expenditure of my earnings — of my houses and my board and my pulpit and my salary and my pulpit-day and morninion — my travel and sabbath-day — the labours of mine — such is the account given in a pamphlet which the Duke of Wellington gave us from Plymouth Rock, which Sir Walter Scott copied from his own mouth from our pulpit — Dr. Johnson's pamphlet — all these were written in formless outline, without any foundation.\n",
            "\n",
            "All this was new to me — all this was inexplicable and yet I held on to it, till I found that the author was Mr. Coventry.\n",
            "\n",
            "I have known no man, in all my life, as he has behaved towards me in this – – –– — — — — — — — — — — — — — — — — — Mr. Coventry.\n",
            "\n",
            "I have no words to express my profound surprise and astonishment — no word which I dare suggest to you. I hope that my words are understood — if I have not said them honestly.\n",
            "\n",
            "In my early youth I was very religious. I never lost sight of two objects in the life — of one I never knew how to look on another — and the other I never know how to feel on that I ought to love.\n",
            "\n",
            "I never lost sight of the fact that you were born — and that I was your only — but that is no great loss, as though time and death had thrown them entirely into the dust. On the contrary, I rejoice in the fact that they are yours.\n",
            "\n",
            "When my mother died, my faith was shaken by her passing. On her death, my faith was strengthened by the fact that you will die the same way.\n",
            "\n",
            "In the course of the next few hours I read four or five books on Mrs. Halsbury — one or two which concern Mrs. — — .’s passing. The next one is a book on Sir Thomas More — who had two sons, one of whom died in action. ’s two youngest, was a most distinguished lawyer and statesman. ’s father was a member of this House.\n",
            "\n",
            "At last, after the most pressing anxieties about my mother and the passing of almost two years, I found a book which I shall treasure in my library as a sacred treasure, containing not only the thoughts and feelings of the writer, but also some of the words of ’s late husband.\n",
            "\n",
            "You will be surprised at the variety and the variety of subjects — of almost every class, rank and sex — covered in that four pages!\n",
            "\n",
            "On my return from Bordeaux, I read to a friend a book on Voltaire — another on Proudhon — yet there is scarcely space to mention but a few.\n",
            "\n",
            "There are other books which, although they concern me as little as possible, still make me sentimental, that is to say, inspire in me a deep desire to do what may for ’s safety and yours a like desire to do ’t the same.\n",
            "\n",
            "There are, of course, ’much ’sss in these lines by Mr. Ruskin, which a number of you may know, and a few words in ‘s Sunday Herald for which I am eternally grateful.\n",
            "\n",
            "It was on those cold, grey, wet, gloomy, and gloomy Sunday afternoons, long since, when, against the bleak backdrop of those bleak, coldly shining London streets, I saw and knew the same dreadful, deadly, hopeless and hopelessly beautiful things which I now see and know to be the mark and memorial of the favor and care and love which life has shown me here and hereafter, I owe the\n",
            "\n",
            "[410 | 447.14] loss=0.31 avg=2.12\n",
            "[420 | 455.99] loss=0.75 avg=2.08\n",
            "[430 | 464.85] loss=0.44 avg=2.03\n",
            "[440 | 473.70] loss=0.49 avg=1.99\n",
            "[450 | 482.54] loss=0.79 avg=1.95\n",
            "[460 | 491.38] loss=0.94 avg=1.93\n",
            "[470 | 500.22] loss=0.46 avg=1.89\n",
            "[480 | 509.07] loss=0.46 avg=1.85\n",
            "[490 | 517.90] loss=0.55 avg=1.82\n",
            "[500 | 526.75] loss=0.26 avg=1.78\n",
            "Saving checkpoint/run1/model-500\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "266704b9-c7fa-4d50-9f6d-b92e107ad05b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run1/model-500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "3f779599-a7a5-4424-ebab-8a0e23f8865e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In this article, we will be discussing a set of guidelines, which reduce the amount of time required to code an application. These guidelines are conceived of as a set of requirements, which can be applied in any way.\n",
            "\n",
            "We will be using Python as a programming language. We are not considering any language as a programming tool, but rather as a language which, most of the time, can be used in any kind of applications.\n",
            "\n",
            "I would like to tell you that this is not a set of rules, but rather a set of guidelines, which can be applied in any way.\n",
            "\n",
            "Well, here they are:\n",
            "\n",
            "1) Use of the python package\n",
            "\n",
            "We cannot apply a set of guidelines, which are precisely the same, under the same conditions, for all numerical computations.\n",
            "\n",
            "For example, for the case of a matrix multiplication we must apply all the same rules, to all the same digits, but for addition the rules will be different.\n",
            "\n",
            "This is because there are three different ways to multiply a given number. 2) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number. 3) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number.\n",
            "\n",
            "4) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number.\n",
            "\n",
            "5) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number.\n",
            "\n",
            "6) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number.\n",
            "\n",
            "7) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number.\n",
            "\n",
            "8) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number.\n",
            "\n",
            "9) Use of the python package is not enough\n",
            "\n",
            "We can not apply the same rules, only the different ones, to all the same functions.\n",
            "\n",
            "For example, for the case of the matrix multiplication we cannot apply the same rules, to all the same digits, but for addition, in addition to all the rules, we can use the different ones, that is, all the different functions.\n",
            "\n",
            "This is because there are three different ways to multiply a given number.\n",
            "\n",
            "10) Use of the python package is not enough\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "d1d7b76a-835b-4698-c2c8-837c069bd416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "text = gpt2.generate(sess,\n",
        "              length=500,\n",
        "              temperature=0.5,\n",
        "              prefix=\"Dear\",\n",
        "              nsamples=5,\n",
        "              return_as_list=True,\n",
        "              batch_size=5,\n",
        "              truncate='<|endoftext|>'\n",
        "              )\n",
        "\n",
        "# for i in range(len(text)):\n",
        "#   print(text[i])\n",
        "#   print()\n",
        "  \n",
        "letters = text[0]\n",
        "\n",
        "final_letters = letters.split('\\n\\n\\n')\n",
        "\n",
        "for i in range (len(final_letters)):\n",
        "  print(final_letters[i])\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dear, how lovely it is to see you, dear, so tender, so dear. You are my Rose, my all, and my all to you is sweet.\n",
            "Your most obedient,\n",
            "John Keats\n",
            "My dear Friend,\n",
            "I have nothing but praise for you, nothing but motives for encouraging you, nothing but means for securing your happiness but honesty, goodness, and security. I have been long in loathing myself for ever a sort of moral blindness, like that of which Plato speaks in his Metamorphoses: blind indeed, because I have been ignorant of myself, and therefore I love not; yea, I have been foolishly proud, because I have looked upon myself as THINGS, rather THANNESS and DIVINITY; nay, THINGS rather THANME, than THINKING and PHRASES; nay, THINKING and PHRASES than THINKING and DESPISES; nay, THINKING and DESPISES than THINKING and LUST rather THINKLING and LUSTINESS than THINKLING and LUSTINESS; than I have been conceitedly proud; than I have loved the thought of myself rather THAN THINKING and THINKING and THINKLING and THINKLING than THINKLING and LUST and LUSTINESS.\n",
            "But let me have all the glory of a wooer for a year or two, and I will admit that I am master of my thoughts, and the thoughts are but the thoughts: then will I yield up the question, and subscribe myself his slave, and serve him for a friend.\n",
            "For my part, I declare myself ready to give everything to the mutual interest of our two countries, and our two peoples, for the support of which we owe so many lives and fortunes.\n",
            "Give, and you shall have; and if you cannot give, yet let it not be long before we discover a way of redeeming from one another's debt, by force if necessary, that which we owe to one another.\n",
            "This is my principal object in this letter. To set before you, as briefly and simply as I may, the consciousness of the immense importance which lies upon me in relation to you an immediate and absolute decision as to the object on which I am about to place myself. My resolution is, That whatever decision you shall make for us, it should be a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}